{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb0f07e5-e1c8-42c5-a7bf-f021baa8bea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 20.5/44.4 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------------------------------- 44.4/44.4 kB 550.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\brian.baert\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\brian.baert\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\brian.baert\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\brian.baert\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\brian.baert\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\brian.baert\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.0-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\brian.baert\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\brian.baert\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\brian.baert\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\brian.baert\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\brian.baert\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\brian.baert\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\brian.baert\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\brian.baert\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/9.9 MB 6.7 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.5/9.9 MB 6.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.8/9.9 MB 8.3 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.3/9.9 MB 9.2 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.8/9.9 MB 9.3 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.2/9.9 MB 9.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.3/9.9 MB 9.2 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.7/9.9 MB 9.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.1/9.9 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.5/9.9 MB 9.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.9/9.9 MB 9.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.3/9.9 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.6/9.9 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.0/9.9 MB 9.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.3/9.9 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.6/9.9 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.0/9.9 MB 9.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.5/9.9 MB 9.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.9/9.9 MB 9.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.4/9.9 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.8/9.9 MB 9.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.2/9.9 MB 9.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.9 MB 9.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.9 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.0/9.9 MB 9.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.4/9.9 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.9/9.9 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.9/9.9 MB 8.8 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "   ---------------------------------------- 0.0/436.4 kB ? eta -:--:--\n",
      "   ------------------------------ -------- 337.9/436.4 kB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------- 436.4/436.4 kB 13.7 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "   ---------------------------------------- 0.0/286.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 286.3/286.3 kB 17.3 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.20.0-cp312-none-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.3 MB 9.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.9/2.3 MB 11.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.2/2.3 MB 10.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.3 MB 10.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.1/2.3 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 10.5 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.25.1 safetensors-0.4.5 tokenizers-0.20.0 transformers-4.45.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "475ec55a-8782-4941-895f-946a0adcc6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6771e859-b06f-4be8-a5a4-93f85040148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_MODEL_NAME = 'bert-base-cased'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = BertTokenizer.from_pretrained(PT_MODEL_NAME)\n",
    "EPOCHS = 10\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "616b275c-28f1-4ecd-86df-78314f50a057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msg</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nah think go usf life around though</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 msg  spam\n",
       "0  go jurong point crazy available bugis n great ...     0\n",
       "1                            ok lar joking wif u oni     0\n",
       "2  free entry wkly comp win fa cup final tkts st ...     1\n",
       "3                u dun say early hor u c already say     0\n",
       "4                nah think go usf life around though     0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/animesharma3/SPAM-SMS-Detection/master/spam_sms_collection.csv')[['msg', 'spam']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ec2909a-159d-4188-b887-02230e489912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "329d668a-f823-451d-8484-35a4c8888668",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMSCollectionDataset(Dataset):\n",
    "    def __init__(self, spam, msgs, tokenizer, max_len):\n",
    "        self.msgs = msgs\n",
    "        self.spam = spam\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.msgs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        msg = str(self.msgs[i])\n",
    "        spam = self.spam[i]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            msg, \n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'msg': msg,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'spam': torch.tensor(spam, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e9adf342-e9c5-4a46-9aa8-f568daf6ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = SMSCollectionDataset(\n",
    "        spam=df['spam'].to_numpy(),\n",
    "        msgs=df['msg'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(ds,batch_size=batch_size,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "50fd4ce7-3884-4442-b705-3d9295a8c1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4457, 2), (558, 2), (557, 2))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "df_val, df_test = train_test_split(\n",
    "    df_test,\n",
    "    test_size=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "df_train.shape, df_test.shape, df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cebd1a72-91fd-487d-b0d2-c085c9b094f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f45f975-5fcf-4295-8f31-8567c6c6bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = next(iter(train_data_loader))\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59fd551-8473-4f4b-be35-046ab4d4a254",
   "metadata": {},
   "outputs": [],
   "source": [
    "d['input_ids'].shape, d['attention_mask'].shape, d['spam'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085b118-233a-4133-be6e-1625b2d4608f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
